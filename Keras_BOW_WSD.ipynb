{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            SENTENCE CATEGORY\n",
      "0  Tidak menghalang Kaka untuk meneruskan karier ...  manisan\n",
      "1  Pelakon Sasqia Dahuri yang sah menjadi isteri ...  manisan\n",
      "2  Sebagai kafe yang mengutamakan status halal, k...  manisan\n",
      "3  Suspek, yang bekerja sebagai penjual madu dita...  manisan\n",
      "4  Batu-bata dilekat menggunakan bancuhan campura...  manisan\n",
      "1686\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('madu.csv')\n",
    "df = df[pd.notnull(df['CATEGORY'])]\n",
    "print(df.head(5))\n",
    "print(df['SENTENCE'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_tags = ['manisan','manusia']\n",
    "plt.figure(figsize=(10,4))\n",
    "df.CATEGORY.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartawan BH memenangi Anugerah Kewartawanan Kesihatan Terbaik 2018 (Akhbar Bahasa Melayu) menerusi rencana bertajuk Madu Diabetes.\n",
      "Tag: manisan\n"
     ]
    }
   ],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['SENTENCE', 'CATEGORY']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Tag:', example[1])\n",
    "\n",
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wartawan bh memenangi anugerah kewartawanan kesihatan terbaik 2018 akhbar bahasa melayu menerusi rencana bertajuk madu diabetes\n",
      "Tag: manisan\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "df['SENTENCE'] = df['SENTENCE'].apply(clean_text)\n",
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1660"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SENTENCE'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.SENTENCE\n",
    "y = df.CATEGORY\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.896551724137931\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    manisan       0.94      0.89      0.91        18\n",
      "    manusia       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.90      0.90      0.90        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.896551724137931\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    manisan       0.94      0.89      0.91        18\n",
      "    manusia       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.90      0.90      0.90        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.896551724137931\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    manisan       0.94      0.89      0.91        18\n",
      "    manusia       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.90      0.90      0.90        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58 samples, validate on 7 samples\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 1s 18ms/step - loss: 0.7041 - acc: 0.5345 - val_loss: 0.6431 - val_acc: 0.8571\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 0s 676us/step - loss: 0.5906 - acc: 0.7586 - val_loss: 0.5772 - val_acc: 0.8571\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 0s 729us/step - loss: 0.4677 - acc: 0.9655 - val_loss: 0.5221 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3819 - acc: 0.9483 - val_loss: 0.4721 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 0s 730us/step - loss: 0.3131 - acc: 1.0000 - val_loss: 0.4269 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 0s 720us/step - loss: 0.2604 - acc: 1.0000 - val_loss: 0.3901 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 0s 858us/step - loss: 0.2204 - acc: 1.0000 - val_loss: 0.3565 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 0s 644us/step - loss: 0.1721 - acc: 1.0000 - val_loss: 0.3289 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.1435 - acc: 1.0000 - val_loss: 0.3048 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 0s 727us/step - loss: 0.1131 - acc: 1.0000 - val_loss: 0.2841 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(df) * .7)\n",
    "train_posts = df['SENTENCE'][:train_size]\n",
    "train_tags = df['CATEGORY'][:train_size]\n",
    "\n",
    "test_posts = df['SENTENCE'][train_size:]\n",
    "test_tags = df['CATEGORY'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 373us/step\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File -> C:\\Users\\ifa\\Project_DeepLearn\\madu_test.txt Predicted label: manusia\n",
      "File -> C:\\Users\\ifa\\Project_DeepLearn\\madu_test2.txt Predicted label: manisan\n"
     ]
    }
   ],
   "source": [
    "# These are the labels we stored from our training\n",
    "# The order is very important here.\n",
    "from pathlib import Path\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# labels = np.array(['alt.atheism', 'comp.graphics'])\n",
    "labels = my_tags\n",
    "test_files = [\"C:\\\\Users\\\\ifa\\\\Project_DeepLearn\\\\madu_test.txt\",\n",
    "              \"C:\\\\Users\\\\ifa\\\\Project_DeepLearn\\\\madu_test2.txt\"]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    "\n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenize.texts_to_matrix(x_data_series, mode='tfidf')\n",
    "\n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.model.save('my_model.h5')\n",
    " \n",
    "# Save Tokenizer i.e. Vocabulary\n",
    "with open('tokenize.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['manisan', 'manusia'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_ #LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File -> C:\\Users\\ifa\\Project_DeepLearn\\madu_test.txt Predicted label: manusia\n",
      "File -> C:\\Users\\ifa\\Project_DeepLearn\\madu_test2.txt Predicted label: manisan\n"
     ]
    }
   ],
   "source": [
    "labels = encoder.classes_\n",
    "test_files = [\"C:\\\\Users\\\\ifa\\\\Project_DeepLearn\\\\madu_test.txt\",\n",
    "              \"C:\\\\Users\\\\ifa\\\\Project_DeepLearn\\\\madu_test2.txt\"]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    "\n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenize.texts_to_matrix(x_data_series, mode='tfidf')\n",
    "\n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence -> Madu Lebih Baik Dari Antibiotik Untuk Rawat Batuk!\n",
      " Predicted label: manisan \n",
      "\n",
      "Sentence -> Ramai nak jadi madu - isteri Lan Solo\n",
      " Predicted label: manusia \n",
      "\n",
      "Sentence -> Madu saya anugerah Allah, kata isteri pertama Predicted label: manusia \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = encoder.classes_\n",
    "\n",
    "with open(\"madu_test.txt\", \"r\") as ins:\n",
    "    x_data = []\n",
    "    for line in ins:\n",
    "        x_data.append(line)\n",
    "\n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenize.texts_to_matrix(x_data_series, mode='tfidf')\n",
    "\n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"Sentence ->\", x_data_series[i], \"Predicted label: \" + predicted_label, \"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
